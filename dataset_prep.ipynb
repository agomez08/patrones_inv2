{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frequent-sociology",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "In the [supervised_learning notebook](supervised_learning.ipynb), we compare and analyze the results for different types of models applied to a supervised learning task. The present notebook outlines the basic steps to prepare the dataset for the execution of this learning task.\n",
    "\n",
    "For simplicity and familiarity, we will use the dataset **UK Traffic Accidents** that was used in the assignment 1. For more details: [Assignmet 1 Github Page](https://github.com/agomez08/patrones_inv1).\n",
    "\n",
    "As a goal, we plan on using this accidents data to attempt to **predict the severity of the accident given the relevant features in the dataset**. We will start with an exported CSV file from the pre-processing applied in the previous assignment called *'uk_accidents_processed.csv'*. We will take this pre-processing further now that there is a specific application in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-discipline",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing relevant python modules\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read exported data from previous investigation\n",
    "df_accidents_05_14 = pd.read_csv('dataset/uk_accidents_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-compact",
   "metadata": {},
   "source": [
    "Below we identify the number of instances for each of the possible severities. From the dataset documentation and the changes applied on the pre-processing already applied, this is what each of the numbers mean:\n",
    "\n",
    "1 - Slight severity.\n",
    "\n",
    "2 - Serious severity.\n",
    "\n",
    "3 - Fatal severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by each type\n",
    "df_accidents_05_14['Accident_Severity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-occasions",
   "metadata": {},
   "source": [
    "## Categories Reduction\n",
    "\n",
    "To simplify the prediction task and to start balancing the classes, we will reduce the scope of this experiment to attempt to identify an accident as Severe or Non-Severe:\n",
    "\n",
    "0 - Non-Severe (Slight severity).\n",
    "\n",
    "1 - Severe (Serious or Fatal severity).\n",
    "\n",
    "We apply these changes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 1 as 0\n",
    "df_accidents_05_14.loc[df_accidents_05_14['Accident_Severity'] == 1, 'Accident_Severity'] = 0\n",
    "\n",
    "# Save 2 and 3 as 1\n",
    "df_accidents_05_14.loc[df_accidents_05_14['Accident_Severity'] == 2, 'Accident_Severity'] = 1\n",
    "df_accidents_05_14.loc[df_accidents_05_14['Accident_Severity'] == 3, 'Accident_Severity'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column to be more representative:\n",
    "df_accidents_05_14 = df_accidents_05_14.rename(columns={'Accident_Severity': 'Severe_Accident'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-cursor",
   "metadata": {},
   "source": [
    "With this change we have the classes distribution shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by each type\n",
    "df_accidents_05_14['Severe_Accident'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-title",
   "metadata": {},
   "source": [
    "## Balancing Classes\n",
    "\n",
    "To balance the classes, and since we have a very high-number of instances anyways, we will down-sample the *Non-Severe* class to have the same number of samples as the *Severe* class. For this purpose we use the *resample* function of *sklearn* which will apply a random method for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample non-severe category\n",
    "df_accidents_non_severe = df_accidents_05_14[df_accidents_05_14['Severe_Accident'] == 0] \n",
    "df_accidents_non_severe = sklearn.utils.resample(df_accidents_non_severe, replace=False,\n",
    "                                                 n_samples=71178, random_state=18)\n",
    " \n",
    "# Combine again\n",
    "df_accidents_severe = df_accidents_05_14[df_accidents_05_14['Severe_Accident'] == 1]\n",
    "df_accidents = pd.concat([df_accidents_non_severe, df_accidents_severe])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-incidence",
   "metadata": {},
   "source": [
    "As we can see below, the two classes are now balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by each type\n",
    "df_accidents['Severe_Accident'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-organize and then reset index\n",
    "df_accidents.sort_index(inplace=True)\n",
    "df_accidents.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-surrey",
   "metadata": {},
   "source": [
    "## Dropping Irrelevant Features\n",
    "\n",
    "Below we proceeed to remove different columns that due to their nature are not considered relevant for the application we have in mind for this data. Some of them are also consequences of the accidents which are highly related to the severity and wouldn't be available for the prediction (*'Number_of_Vehicles'* and *'Number_of_Casualties'*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents.drop(inplace=True, columns=['Police_Force', 'Number_of_Vehicles', 'Number_of_Casualties', '1st_Road_Class',\n",
    "                                        '1st_Road_Number', '2nd_Road_Class', '2nd_Road_Number', 'Police_Attended',\n",
    "                                         'Year', 'Local_Authority_Highway'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-fitting",
   "metadata": {},
   "source": [
    "## Storing Resultant Dataset\n",
    "Below we present a glance of the obtained dataset that will be used to train the supervised learning models. As we can observe it has 142356 instances and 58 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to separate file called uk_accidents_for_sev_prediction.csv\n",
    "df_accidents.to_csv('dataset/uk_accidents_for_sev_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
