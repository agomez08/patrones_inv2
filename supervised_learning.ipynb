{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compressed-arrival",
   "metadata": {},
   "source": [
    "# Supervised Learning Models\n",
    "\n",
    "In this notebook, we present a series of experiments to compare different models that rely on supervised learning for a classification task. \n",
    "\n",
    "We will rely on a dataset that contains instances of **UK Traffic Accidents** to attempt to **predict the severity of the accident given the relevant features in the dataset**. For more details on the preparation of the dataset used here consult the [dataset_prep notebook](dataset_prep.ipynb).\n",
    "\n",
    "Before evaluating different models, we will introduce the metrics that will be utilized by this purpose. \n",
    "**WHAT ELSE?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-testimony",
   "metadata": {},
   "source": [
    "## Classification Task Metrics\n",
    "\n",
    "In this section we present the metrics that will be utilized to compare the different models for classification.\n",
    "\n",
    "**Note:** For more details on the definition of the metrics presented below refer to the scikit learn documentation: [sklearn.metrics Documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "\n",
    "To better understand these metrics, we are going to consider a very simple example related to a classification task. Let's say we have a model trained to identify if the animal in a given picture is or not a dog. The model is evaluated with 100 pictures, from which 40 do contain a dog and 60 do not. From the 40 pictures with a dog, the algorithm correctly classifies 35 of them as containing a dog. From the 60 that do not contain a dog, the algorithm incorrectly classisifes 10 of them as containing a dog.\n",
    "\n",
    "* 100 pictures\n",
    "  * 40 have a dog.\n",
    "    * Model predicts 35 as having a dog.\n",
    "    * Model predicts 5 as not having a dog.\n",
    "  * 60 do not have a dog.\n",
    "    * Model predicts 10 as having a dog.\n",
    "    * Model predicts 50 as not having a dog.\n",
    "\n",
    "\n",
    "### Classification Accuracy\n",
    "\n",
    "The simplest and most widely used method. Represents the ratio of correct to the total number of predictions. Normally presented as a percentage:\n",
    "\n",
    "$$Accuracy (\\%) = 100 * \\frac{correct\\_predictions}{total\\_predictions}$$\n",
    "\n",
    "For our example scenario, the accuracy would be calculated as below:\n",
    "\n",
    "$$Accuracy (\\%) = 100 * \\frac{35 + 50}{100}$$\n",
    "$$Accuracy = 85\\%$$\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "A matrix or table that aids visualizing the accuracy of the classification by comparing the *'Truth'* values against the predictions done by the model in a row against column comparison. This will be better understood with the data of our example scenario.\n",
    "\n",
    "In the rows, we will present the count of predictions done by the model for each of the categories. For the columns, we will present the *'Truth'* count for each of them.\n",
    "\n",
    "|                        | non-dog Truth | dog Truth |\n",
    "|------------------------|---------------|-----------|\n",
    "| **non-dog prediction** |       50      |     5     |\n",
    "| **dog  prediction**    |       10      |     35    |\n",
    "\n",
    "A Confusion Matrix from a good classifier will tend to have high numbers in the main diagonal (correct classifications) and low numbers elsewhere. \n",
    "\n",
    "### Sensitivity, Recall or True Positive Rate\n",
    "\n",
    "Aids determining the ability of the model to correctly identify *'True'* values correctly in binary classifications. It can be calculated as the ratio of *'True'* values classified as such (true_positives) to the total number of *'True'* values (true_positives and false_negatives).\n",
    "\n",
    "Normally presented as a percentage:\n",
    "\n",
    "$$Sensitivity (\\%) = 100 * \\frac{true\\_positives}{true\\_positives + false\\_negatives}$$\n",
    "\n",
    "For our example scenario, the sensitivity would be calculated as below:\n",
    "\n",
    "$$Sensitivity (\\%) = 100 * \\frac{35}{35 + 5}$$\n",
    "$$Sensitivity = 87.5\\%$$\n",
    "\n",
    "\n",
    "### Specificity or True Negative Rate\n",
    "\n",
    "Aids determining the ability of the model to correctly identify *'False'* values correctly in binary classifications. It can be calculated as the ratio of *'False'* values classified as such (true_negatives) to the total number of *'False'* values (true_negatives and false_positives).\n",
    "\n",
    "Normally presented as a percentage:\n",
    "\n",
    "$$Specificity (\\%) = 100 * \\frac{true\\_negatives}{true\\_negatives + false\\_positives}$$\n",
    "\n",
    "For our example scenario, the specificity would be calculated as below:\n",
    "\n",
    "$$specificity (\\%) = 100 * \\frac{50}{50 + 10}$$\n",
    "$$specificity = 83.3\\%$$\n",
    "\n",
    "\n",
    "### Precision\n",
    "\n",
    "It represents the ratio of results correctly identified as *'True'* values to the total number of values classified as *'True'* by the model.\n",
    "\n",
    "Normally presented as a percentage:\n",
    "\n",
    "$$Precision (\\%) = 100 * \\frac{true\\_positives}{true\\_positives + false\\_positives}$$\n",
    "\n",
    "For our example scenario, the precision would be calculated as below:\n",
    "\n",
    "$$Precision (\\%) = 100 * \\frac{35}{35 + 10}$$\n",
    "$$Precision = 77.8\\%$$\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "It aids combining the Precision and Recall into a single metric as a harmonic mean of them.\n",
    "\n",
    "Normally presented as a percentage:\n",
    "\n",
    "$$F1(\\%) = 100 * 2 * \\frac{precision * recall}{precision + recall}$$\n",
    "\n",
    "For our example scenario, the F1-score would be calculated as below:\n",
    "\n",
    "$$F1 (\\%) =  2 * \\frac{77.8 * 87.5}{77.8 + 87.5}$$\n",
    "$$F1 = 82.4\\%$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing relevant python modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import model_selection, neighbors\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents = pd.read_csv('dataset/uk_accidents_for_sev_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_accidents.drop('Severe_Accident', axis=1)\n",
    "target = df_accidents['Severe_Accident']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(features, target, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "neighbors_settings = range(1, 11)\n",
    "for n_neighbors in neighbors_settings:\n",
    "    # se construye el modelo de clasificacion\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # se almacena el \"training set accuracy\"\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    # se almacena la \"generalization accuracy\"\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbg = naive_bayes.GaussianNB().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(nbg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(nbg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tree = tree.DecisionTreeClassifier(max_depth=8, random_state=0)\n",
    "b_tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(b_tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(b_tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(b_tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    imps = []\n",
    "    imp_names = []\n",
    "    for imp, imp_name in zip(model.feature_importances_, features.columns):\n",
    "        if imp > 0.001:\n",
    "            imps.append(imp)\n",
    "            imp_names.append(imp_name)\n",
    "        \n",
    "    n_features = len(imp_names)\n",
    "    plt.barh(range(n_features), imps, align='center')\n",
    "    plt.yticks(np.arange(n_features), imp_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "plot_feature_importances(b_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(rf.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
